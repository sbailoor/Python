{"cells":[{"cell_type":"markdown","metadata":{"id":"m2fFAISlriKt"},"source":["# Project: Bank Marketing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z06R8iYzrmwQ"},"source":["## Step 1: Download the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVH8z-xsr1jJ"},"outputs":[],"source":["# Systematic data loading approach\n","import pandas as pd\n","import numpy as np\n","\n","# Load dataset with comprehensive inspection\n","df = pd.read_csv('bank_marketing_2024.csv')\n","print(f\"Dataset shape: {df.shape}\")\n","\n","# CRITICAL: Remove duration and duration-based features\n","# Duration is only known after the call, making it unusable for prediction\n","print(\"\\nRemoving duration feature (data leakage)...\")\n","if 'duration' in df.columns:\n","    df = df.drop('duration', axis=1)\n","    print(\"Duration column removed\")\n","\n","print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"]},{"cell_type":"markdown","metadata":{"id":"gnk79m4Csjms"},"source":["## Step 2: Business Understanding"]},{"cell_type":"markdown","metadata":{"id":"y58cibQiryI0"},"source":["### Business Case:\n","\n","\n","This project addresses the critical challenge of optimizing direct marketing campaign effectiveness for financial institutions through predictive analytics. By leveraging machine learning to identify high-probability prospects for term deposit subscriptions, we aim to significantly improve campaign ROI, reduce marketing waste, and enhance customer targeting precision.\n"]},{"cell_type":"markdown","metadata":{"id":"85adb093"},"source":["### Project Goal:\n","\n","The primary goal of this project is to develop a predictive model capable of identifying individuals who are most likely to subscribe to a term deposit, based on the provided bank marketing dataset. This model will serve as a tool to optimize future direct marketing campaigns by enabling more targeted outreach, thereby increasing subscription rates and improving overall campaign efficiency and ROI."]},{"cell_type":"markdown","metadata":{"id":"lDw4fneXtvam"},"source":["## Step 3: Data Understanding"]},{"cell_type":"markdown","metadata":{"id":"79a17552"},"source":["### Load data\n","Load the dataset \"bank-additional-full.csv\" into a pandas DataFrame.\n"]},{"cell_type":"markdown","metadata":{"id":"64f00c0c"},"source":["### Explore Data Structure and Basic Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dc3a805"},"outputs":[],"source":["# Display the first few rows\n","print(\"First 5 rows:\")\n","display(df.head())\n","\n","# Display column names and data types\n","print(\"\\nColumn names and data types:\")\n","display(df.info())\n","\n","# Display basic statistics for numerical columns\n","print(\"\\nDescriptive statistics for numerical columns:\")\n","display(df.describe())"]},{"cell_type":"markdown","metadata":{"id":"BEViQFX2i8lj"},"source":["### Variable Documentation\n","\n","Here is a description of each variable in the dataset:\n","\n","| Variable         | Description                                                                   | Type      |\n","|------------------|-------------------------------------------------------------------------------|-----------|\n","| age              | Age of the client.                                                            | numerical |\n","| job              | Type of job                                                  | categorical |\n","| marital          | Marital status.                                                               | categorical |\n","| education        | Level of education.                                                           | categorical |\n","| default          | Has credit in default?                                                        | categorical |\n","| housing          | Has housing loan?                                                             | categorical |\n","| loan             | Has personal loan?                                                            | categorical |\n","| contact          | Contact communication type.                                                   | categorical |\n","| month            | Last contact month of year.                                                   | categorical |\n","| day_of_week      | Last contact day of the week.                                                 | categorical |\n","| duration         | Last contact duration, in seconds.                                            | numerical |\n","| campaign         | Number of contacts performed during this campaign and for this client.          | numerical |\n","| pdays            | Number of days that passed after the client was last contacted from a previous campaign. (999 means client was not previously contacted) | numerical |\n","| previous         | Number of contacts performed before this campaign and for this client.          | numerical |\n","| poutcome         | Outcome of the previous marketing campaign.                                   | categorical |\n","| emp_var_rate     | Employment variation rate - quarterly indicator.                              | numerical |\n","| cons_price_idx   | Consumer price index - monthly indicator.                                     | numerical |\n","| cons_conf_idx    | Consumer confidence index - monthly indicator.                                | numerical |\n","| euribor3m        | Euribor 3 month rate - daily indicator.                                       | numerical |\n","| nr_employed      | Number of employees - quarterly indicator.                                    | numerical |\n","| y                | has the client subscribed a term deposit?                                      | binary    |\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1363b754"},"source":["## Step 4: Exploratory Data Analysis (EDA)\n","Perform a structural assessment of the dataset \"bank-additional-full.csv\" by analyzing data types, auditing missing values, detecting duplicates, and validating data ranges."]},{"cell_type":"markdown","metadata":{"id":"5d15879b"},"source":["### Data types analysis\n","\n","Verify the data types of each column and identify numerical and categorical variables.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ef31792"},"outputs":[],"source":["# Identify numerical and categorical columns\n","numerical_columns = df.select_dtypes(include=np.number).columns.tolist()\n","categorical_columns = df.select_dtypes(include='object').columns.tolist()\n","\n","# Print the identified columns\n","print(\"\\nNumerical columns:\", numerical_columns)\n","print(\"Categorical columns:\", categorical_columns)"]},{"cell_type":"markdown","metadata":{"id":"a835efa9"},"source":["### Missing value audit\n","\n","Calculate and display the number and percentage of missing values for each column.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7f449e81"},"outputs":[],"source":["missing_values_count = df.isnull().sum()\n","missing_values_percentage = (missing_values_count / len(df)) * 100\n","\n","missing_values_df = pd.DataFrame({\n","    'Missing Count': missing_values_count,\n","    'Missing Percentage (%)': missing_values_percentage\n","})\n","\n","print(\"Missing values audit:\")\n","display(missing_values_df)"]},{"cell_type":"markdown","metadata":{"id":"40a6c954"},"source":["### Duplicate detection\n","\n","Identify and quantify the number of duplicate rows in the dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad52c44e"},"outputs":[],"source":["duplicate_rows_count = df.duplicated().sum()\n","print(f\"Total number of duplicate rows: {duplicate_rows_count}\")"]},{"cell_type":"markdown","metadata":{"id":"c153cd9d"},"source":["### Data range validation\n","\n","For numerical columns, check for outliers and impossible values using descriptive statistics and visualizations (e.g., box plots, histograms).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2b903fb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","numerical_columns = df.select_dtypes(include=np.number).columns.tolist()\n","\n","for col in numerical_columns:\n","    # Box plot\n","    plt.figure(figsize=(10, 4))\n","    sns.boxplot(x=df[col])\n","    plt.title(f'Box Plot of {col}')\n","    plt.xlabel(col)\n","    plt.show()\n","\n","    # Histogram\n","    plt.figure(figsize=(10, 4))\n","    sns.histplot(data=df, x=col, kde=True)\n","    plt.title(f'Histogram of {col}')\n","    plt.xlabel(col)\n","    plt.ylabel('Frequency')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4bab1a57"},"source":["### Data Analysis Key Findings\n","\n","*   The dataset contains a mix of `int64`, `float64`, and `object` data types.\n","*   There are no missing values in any of the columns or duplicate rows in the dataset.\n","*   Visualizations (box plots and histograms) for numerical columns were generated to help identify potential outliers and assess data ranges.\n","\n","### Insights or Next Steps\n","\n","*   Further analyze the box plots and histograms of the numerical columns to specifically identify and investigate potential outliers or impossible values based on domain knowledge.\n"]},{"cell_type":"markdown","metadata":{"id":"5b4e3143"},"source":["## Step 5: Univariate Analysis\n","Perform univariate analysis on the dataset."]},{"cell_type":"markdown","metadata":{"id":"6775963e"},"source":["### Target variable distribution\n","\n","Analyze the distribution of the target variable 'y' (subscription to a term deposit) by calculating and visualizing the baseline conversion rate (yes/no ratio).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09b833a4"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Calculate value counts and percentages of the target variable 'y'\n","target_counts = df['y'].value_counts()\n","target_percentages = df['y'].value_counts(normalize=True) * 100\n","\n","print(\"Value counts of the target variable 'y':\")\n","display(target_counts)\n","\n","print(\"\\nPercentage of the target variable 'y':\")\n","display(target_percentages)\n","\n","# Create a count plot of the target variable 'y'\n","plt.figure(figsize=(6, 4))\n","sns.countplot(data=df, x='y', palette='viridis')\n","plt.title('Distribution of Target Variable (Subscription to Term Deposit)')\n","plt.xlabel('Subscribed to Term Deposit (y)')\n","plt.ylabel('Count')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fe9af0ff"},"source":["### Categorical variables\n","\n","Explore the frequency distributions and assess the balance of categories for each categorical variable using count plots or bar plots.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b01c9bc9"},"outputs":[],"source":["categorical_columns = df.select_dtypes(include='object').columns.tolist()\n","\n","for col in categorical_columns:\n","    print(f\"\\nAnalysis of column: {col}\")\n","\n","    # Calculate value counts and percentages\n","    value_counts = df[col].value_counts()\n","    value_percentages = df[col].value_counts(normalize=True) * 100\n","\n","    print(\"\\nValue counts:\")\n","    display(value_counts)\n","\n","    print(\"\\nPercentage of values:\")\n","    display(value_percentages)\n","\n","    # Create a count plot\n","    plt.figure(figsize=(12, 6))\n","    sns.countplot(data=df, x=col, hue=col, palette='viridis', order=value_counts.index, legend=False)\n","    plt.title(f'Distribution of {col}')\n","    plt.xlabel(col)\n","    plt.ylabel('Count')\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e15138f5"},"source":["### Numerical variables\n","\n","Generate statistical summaries (mean, median, standard deviation, min, max, quartiles) and visualize the distributions using histograms and box plots to detect outliers and understand the spread of the data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"400c8920"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","numerical_columns = df.select_dtypes(include=np.number).columns.tolist()\n","\n","for col in numerical_columns:\n","    print(f\"\\nAnalysis of numerical column: {col}\")\n","\n","    # Descriptive statistics\n","    print(\"\\nDescriptive Statistics:\")\n","    display(df[col].describe())\n","\n","    # Box plot\n","    plt.figure(figsize=(10, 4))\n","    sns.boxplot(x=df[col])\n","    plt.title(f'Box Plot of {col}')\n","    plt.xlabel(col)\n","    plt.show()\n","\n","    # Histogram\n","    plt.figure(figsize=(10, 4))\n","    sns.histplot(data=df, x=col, kde=True)\n","    plt.title(f'Histogram of {col}')\n","    plt.xlabel(col)\n","    plt.ylabel('Frequency')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0c067d10"},"source":["### Temporal patterns\n","\n","Analyze the distribution of campaign timings by month and day of the week to identify any patterns or trends.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"116667d2"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 1. Create a count plot of the 'month' column\n","plt.figure(figsize=(12, 6))\n","sns.countplot(data=df, x='month', hue='month', palette='viridis', order=['mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'], legend=False)\n","# 2. Add a title and labels to the month count plot\n","plt.title('Distribution of Contacts by Month')\n","plt.xlabel('Month')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","# 3. Display the month count plot\n","plt.show()\n","\n","# 4. Create a count plot of the 'day_of_week' column\n","plt.figure(figsize=(8, 6))\n","sns.countplot(data=df, x='day_of_week', hue='day_of_week', palette='viridis', order=['mon', 'tue', 'wed', 'thu', 'fri'], legend=False)\n","# 5. Add a title and labels to the day of the week count plot\n","plt.title('Distribution of Contacts by Day of the Week')\n","plt.xlabel('Day of the Week')\n","plt.ylabel('Count')\n","# 6. Display the day of the week count plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"17a051f2"},"source":["### Data Analysis Key Findings (Univariate Analysis)\n","\n","*   The target variable 'y' (subscription to a term deposit) is imbalanced, with approximately 74.98% of instances being 'no' and 25.02% being 'yes'.\n","*   The univariate analysis on categorical variables revealed the frequency distribution and balance (or imbalance) of categories within each feature. Some categories like 'default' are highly imbalanced.\n","*   Statistical summaries and visualizations of numerical variables provided insights into their spread, central tendency, and potential outliers. Variables like 'duration', 'campaign', and 'pdays' show skewed distributions and potential outliers.\n","*   Analysis of temporal patterns showed the distribution of campaign contacts across different months and days of the week, with 'may' having the highest number of contacts and 'dec', 'mar', 'oct', 'sep' having relatively lower contact counts.\n","\n","### Insights or Next Steps\n","\n","*   Given the class imbalance in the target variable, consider using techniques like oversampling or undersampling during model training or using evaluation metrics appropriate for imbalanced datasets (e.g., F1-score, Precision, Recall, AUC).\n","*   Further investigate the distributions and potential outliers in the numerical variables identified during the analysis, such as 'duration', 'campaign', and 'pdays', as they might require transformation or special handling.\n","*   Consider the implications of highly imbalanced categorical features like 'default' on model training."]},{"cell_type":"markdown","metadata":{"id":"15f379ea"},"source":["## Step 6: Bivariate/Multivariate Analysis\n","Perform bivariate and multivariate analysis on the dataset to understand the relationships between variables and the target variable 'y'."]},{"cell_type":"markdown","metadata":{"id":"baa22394"},"source":["### Correlation matrix\n","\n","Calculate and visualize the correlation matrix for numerical variables to identify multicollinearity issues and relationships between features.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8457678a"},"outputs":[],"source":["numerical_df = df.select_dtypes(include=np.number)\n","\n","correlation_matrix = numerical_df.corr()\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Matrix of Numerical Variables')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"344834f5"},"source":["### Target variable relationships (categorical)\n","\n","Use Chi-square tests or other appropriate methods to analyze the relationship between each categorical variable and the target variable 'y'.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"554a0c32"},"outputs":[],"source":["from scipy.stats import chi2_contingency\n","\n","categorical_columns = df.select_dtypes(include='object').columns.tolist()\n","categorical_columns.remove('y')\n","\n","for col in categorical_columns:\n","    print(f\"\\nAnalyzing relationship between '{col}' and 'y':\")\n","\n","    # Create contingency table\n","    contingency_table = pd.crosstab(df[col], df['y'])\n","    print(\"\\nContingency Table:\")\n","    display(contingency_table)\n","\n","    # Perform Chi-square test\n","    chi2, p, dof, expected = chi2_contingency(contingency_table)\n","\n","    print(f\"\\nChi-square statistic: {chi2:.4f}\")\n","    print(f\"P-value: {p:.4f}\")\n","    print(f\"Degrees of freedom: {dof}\")\n","\n","    # Interpret the p-value\n","    alpha = 0.05\n","    if p < alpha:\n","        print(f\"Interpretation: There is a statistically significant relationship between '{col}' and 'y' (reject null hypothesis).\")\n","    else:\n","        print(f\"Interpretation: There is no statistically significant relationship between '{col}' and 'y' (fail to reject null hypothesis).\")"]},{"cell_type":"markdown","metadata":{"id":"523f3e8e"},"source":["### Target variable relationships (numerical)\n","\n","Use t-tests or other appropriate methods to analyze the relationship between each numerical variable and the target variable 'y'.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2e566a2d"},"outputs":[],"source":["from scipy.stats import ttest_ind\n","\n","# Separate the DataFrame based on the target variable 'y'\n","df_yes = df[df['y'] == 'yes']\n","df_no = df[df['y'] == 'no']\n","\n","# Identify numerical columns\n","numerical_columns = df.select_dtypes(include=np.number).columns.tolist()\n","\n","# Perform independent samples t-test for each numerical column\n","print(\"Independent Samples t-tests for Numerical Variables vs. Target Variable 'y':\")\n","for col in numerical_columns:\n","    # Check if there are enough samples in both groups to perform the t-test\n","    if len(df_yes[col].dropna()) > 1 and len(df_no[col].dropna()) > 1:\n","        ttest_result = ttest_ind(df_yes[col].dropna(), df_no[col].dropna(), equal_var=False) # Welch's t-test (assuming unequal variances)\n","        print(f\"\\nColumn: {col}\")\n","        print(f\"  T-statistic: {ttest_result.statistic:.4f}\")\n","        print(f\"  P-value: {ttest_result.pvalue:.4f}\")\n","\n","        # Interpret the p-value\n","        alpha = 0.05\n","        if ttest_result.pvalue < alpha:\n","            print(f\"  Interpretation: There is a statistically significant difference in the mean of '{col}' between 'yes' and 'no' groups (reject null hypothesis).\")\n","        else:\n","            print(f\"  Interpretation: There is no statistically significant difference in the mean of '{col}' between 'yes' and 'no' groups (fail to reject null hypothesis).\")\n","    else:\n","        print(f\"\\nColumn: {col}\")\n","        print(f\"  Not enough data in one or both groups to perform t-test.\")\n"]},{"cell_type":"markdown","metadata":{"id":"b6b92afc"},"source":["### Economic indicator correlations\n","\n","Analyze the correlations between the economic indicators (`emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m`, `nr.employed`) and the target variable 'y', and potentially with other relevant features.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1d8cf8f"},"outputs":[],"source":["economic_indicators = ['emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed']\n","economic_df = df[economic_indicators + ['y']].copy()\n","\n","economic_df['y_numeric'] = economic_df['y'].apply(lambda x: 1 if x == 'yes' else 0)\n","\n","economic_correlation_matrix = economic_df[economic_indicators + ['y_numeric']].corr()\n","\n","print(\"Correlation matrix of economic indicators and target variable:\")\n","display(economic_correlation_matrix)"]},{"cell_type":"markdown","metadata":{"id":"947910f8"},"source":["### Campaign history impact\n","\n","Analyze the impact of previous campaign history (`pdays`, `previous`, `poutcome`) on the target variable 'y'.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ece56bf0"},"outputs":[],"source":["# 1. Calculate and display the value counts and percentages of the 'poutcome' column.\n","print(\"Value counts of 'poutcome':\")\n","display(df['poutcome'].value_counts())\n","print(\"\\nPercentage of 'poutcome' values:\")\n","display(df['poutcome'].value_counts(normalize=True) * 100)\n","\n","# 2. Create a grouped bar plot showing the distribution of 'poutcome' for each category of the target variable 'y'.\n","plt.figure(figsize=(8, 5))\n","sns.countplot(data=df, x='poutcome', hue='y', palette='viridis')\n","plt.title('Distribution of Previous Campaign Outcome by Subscription Status')\n","plt.xlabel('Previous Campaign Outcome')\n","plt.ylabel('Count')\n","plt.show()\n","\n","# 3. Create box plots to visualize the distributions of 'pdays' and 'previous' for each category of the target variable 'y'.\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","sns.boxplot(data=df, x='y', y='pdays', hue='y', palette='viridis', legend=False)\n","plt.title('Distribution of pdays by Subscription Status')\n","plt.xlabel('Subscribed to Term Deposit (y)')\n","plt.ylabel('Days since last contact (pdays)')\n","\n","plt.subplot(1, 2, 2)\n","sns.boxplot(data=df, x='y', y='previous', hue='y', palette='viridis', legend=False)\n","plt.title('Distribution of previous contacts by Subscription Status')\n","plt.xlabel('Subscribed to Term Deposit (y)')\n","plt.ylabel('Number of previous contacts (previous)')\n","plt.tight_layout()\n","plt.show()\n","\n","\n","# 4. Calculate and display the mean values of 'pdays' and 'previous' for each category of the target variable 'y'.\n","print(\"\\nMean of 'pdays' and 'previous' by Subscription Status:\")\n","display(df.groupby('y')[['pdays', 'previous']].mean())"]},{"cell_type":"markdown","metadata":{"id":"ae1ef4ea"},"source":["### Visualize relationships\n","\n","Create visualizations (e.g., bar plots, box plots, scatter plots) to illustrate the relationships between key variables and the target variable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6c4ac330"},"outputs":[],"source":["# Create a bar plot to visualize the relationship between 'job' and the target variable 'y'.\n","plt.figure(figsize=(14, 7))\n","sns.countplot(data=df, x='job', hue='y', palette='viridis')\n","plt.title('Subscription Count by Job Type')\n","plt.xlabel('Job Type')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n","\n","# Create a bar plot to visualize the relationship between 'marital' and the target variable 'y'.\n","plt.figure(figsize=(10, 6))\n","sns.countplot(data=df, x='marital', hue='y', palette='viridis')\n","plt.title('Subscription Count by Marital Status')\n","plt.xlabel('Marital Status')\n","plt.ylabel('Count')\n","plt.show()\n","\n","# Create a box plot to visualize the relationship between 'age' and the target variable 'y'.\n","plt.figure(figsize=(8, 6))\n","sns.boxplot(data=df, x='y', y='age', hue='y', palette='viridis', legend=False)\n","plt.title('Distribution of Age by Subscription Status')\n","plt.xlabel('Subscribed to Term Deposit (y)')\n","plt.ylabel('Age')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5228bef4"},"source":["### Data Analysis Key Findings (Bivariate/Multivariate Analysis)\n","\n","* A statistically significant relationship exists between the target variable 'y' and several categorical variables based on Chi-square tests (p-value < 0.05): `education` (p-value: 0.0000) and `default` (p-value: 0.0349).\n","* The categorical variables `job`, `marital`, `housing`, `loan`, `contact`, `month`, and `day_of_week`, and `poutcome` do not show a statistically significant relationship with the target variable 'y' based on the Chi-square tests (p-values > 0.05).\n","* Statistically significant differences in the means between 'yes' and 'no' subscription groups were found for the numerical variables `age` (p-value: 0.0000) and `cons_conf_idx` (p-value: 0.0000) based on independent samples t-tests. The 'duration' column was removed earlier in the notebook.\n","* The numerical variables `campaign`, `pdays`, `previous`, `emp_var_rate`, `cons_price_idx`, and `euribor3m`, and `nr_employed` do not show a statistically significant difference in their means between the 'yes' and 'no' groups based on independent samples t-tests (p-values > 0.05).\n","* The correlation matrix of economic indicators and the target variable showed weak correlations. `cons_conf_idx` had the highest absolute correlation with the target variable (0.0413).\n","* Analysis of previous campaign history (`poutcome`, `pdays`, `previous`) showed that clients with a 'success' outcome in the previous campaign are more likely to subscribe, as seen in the countplot for `poutcome`.\n","\n","### Insights or Next Steps\n","\n","* Focus on the variables identified as having a statistically significant relationship with the target variable (`education`, `default`, `age`, `cons_conf_idx`) for feature selection and model building.\n","* While some variables didn't show a statistically significant relationship in the bivariate analysis, their potential interactions with other features or non-linear relationships with the target variable should still be considered during feature engineering and model development.\n","* The 'poutcome' variable, despite not showing a statistically significant relationship in the Chi-square test, appears to be an important indicator based on the descriptive analysis and the countplot showing higher 'yes' subscriptions for the 'success' category. This suggests that the Chi-square test might not fully capture the predictive power of this categorical variable, and it should likely be included in the model."]},{"cell_type":"markdown","metadata":{"id":"43e30a67"},"source":["## Step 7: Data Preparation\n"]},{"cell_type":"markdown","metadata":{"id":"4ee8e42f"},"source":["### Feature creation\n","\n","Based on EDA findings and domain knowledge, consider creating new features that could improve model performance (e.g., interaction terms, polynomial features, grouping rare categories).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BZA4sNIxRKH"},"outputs":[],"source":["# Feature Engineering - BEFORE encoding\n","import pandas as pd\n","import numpy as np\n","\n","print(\"Starting feature engineering on raw data...\")\n","\n","# 1. Handle 'unknown' values FIRST (on raw categorical data)\n","categorical_columns = df.select_dtypes(include='object').columns.tolist()\n","if 'y' in categorical_columns:\n","    categorical_columns.remove('y')\n","\n","for col in categorical_columns:\n","    if 'unknown' in df[col].unique():\n","        mode_value = df[col].mode()[0]\n","        df[col] = df[col].replace('unknown', mode_value)\n","        print(f\"Replaced 'unknown' with mode '{mode_value}' in {col}\")\n","\n","# 2. Create previous contact indicator\n","df['was_previously_contacted'] = ((df['pdays'] != 999) | (df['previous'] > 0)).astype(int)\n","\n","# 3. Customer behavior features (WITHOUT duration)\n","df['contact_frequency_score'] = df['campaign'] + df['previous']\n","df['previous_success'] = (df['poutcome'] == 'success').astype(int)\n","\n","# 4. Economic stability index\n","economic_indicators = ['emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed']\n","df['economic_stability_index'] = df[economic_indicators].sum(axis=1)\n","df['risk_environment_score'] = (\n","    df['emp_var_rate'] + df['euribor3m'] + df['nr_employed'] - df['cons_conf_idx']\n",")\n","\n","# 5. Demographic features (on raw categorical data)\n","def get_life_stage(row):\n","    age = row['age']\n","    education = row['education']\n","    marital = row['marital']\n","    job = row['job']\n","\n","    if age < 30:\n","        return f'young_{marital}'\n","    elif age < 55:\n","        if marital == 'married' and education in ['university.degree', 'professional.course']:\n","            return 'middle_aged_married_educated'\n","        else:\n","            return f'middle_aged_{marital}'\n","    else:\n","        return 'retired' if job == 'retired' else 'senior'\n","\n","def get_financial_stability(row):\n","    housing = row['housing']\n","    loan = row['loan']\n","    return f\"housing_{housing}_loan_{loan}\"\n","\n","def get_profession_risk(row):\n","    job = row['job']\n","    high_risk = ['entrepreneur', 'unemployed']\n","    medium_risk = ['blue-collar', 'services', 'self-employed', 'housemaid']\n","    return 'high_risk' if job in high_risk else ('medium_risk' if job in medium_risk else 'low_risk')\n","\n","df['life_stage_category'] = df.apply(get_life_stage, axis=1)\n","df['financial_stability_indicator'] = df.apply(get_financial_stability, axis=1)\n","df['profession_risk'] = df.apply(get_profession_risk, axis=1)\n","\n","# 6. Key interaction terms (only most important ones)\n","df['age_campaign_interaction'] = df['age'] * df['campaign']\n","df['cons_conf_campaign_interaction'] = df['cons_conf_idx'] * df['campaign']\n","\n","print(f\"Feature engineering complete. Shape: {df.shape}\")\n","print(f\"New features added: {df.shape[1] - len(pd.read_csv('bank_marketing_2024.csv').columns)}\")"]},{"cell_type":"markdown","metadata":{"id":"te1CPrr_xRKI"},"source":["### Create Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPNHKo1_xRKI"},"outputs":[],"source":["# Data Preparation - After Feature Engineering, Before Splitting\n","\n","# 1. Encode target variable\n","df['y_encoded'] = (df['y'] == 'yes').astype(int)\n","df = df.drop('y', axis=1)\n","\n","# 2. Identify column types BEFORE encoding\n","numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n","numerical_features.remove('y_encoded')  # Remove target\n","\n","categorical_features = df.select_dtypes(include='object').columns.tolist()\n","\n","print(f\"Numerical features: {len(numerical_features)}\")\n","print(f\"Categorical features: {len(categorical_features)}\")\n","\n","# 3. Handle outliers (on numerical features only)\n","from scipy.stats.mstats import winsorize\n","\n","for col in numerical_features:\n","    df[col] = winsorize(df[col], limits=(0.05, 0.05))\n","\n","# 4. Check for duplicates\n","print(f\"\\nDuplicates before: {df.duplicated().sum()}\")\n","df = df.drop_duplicates()\n","print(f\"Duplicates after: {df.duplicated().sum()}\")\n","\n","print(\"\\nData preparation complete. Ready for train-test split.\")"]},{"cell_type":"markdown","metadata":{"id":"3f9b1bff"},"source":["### Data Preparation Summary\n","\n","### Key Findings\n","\n","* The target variable 'y' was successfully encoded into a binary numerical feature `y_encoded` (0 for 'no', 1 for 'yes').\n","* Numerical features were identified, and outliers were treated using Winsorization at the 5th and 95th percentiles.\n","* No duplicate rows were found before or after dropping duplicates, indicating no duplicates were present in the dataset at this stage.\n","* Categorical and numerical features were identified and are ready for further processing (like one-hot encoding and scaling) as part of a machine learning pipeline.\n","\n","### Insights or Next Steps\n","\n","* The data is prepared for splitting into training, validation, and testing sets, followed by incorporating preprocessing steps (scaling numerical features and encoding categorical features) within a machine learning pipeline."]},{"cell_type":"markdown","metadata":{"id":"e7d5fc16"},"source":["### Review and refine features\n","\n","Assess the newly created features for relevance and potential issues (e.g., multicollinearity).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"391c51d3"},"outputs":[],"source":["# Identify numerical columns (excluding the target variable)\n","numerical_columns = df.select_dtypes(include=np.number).columns.tolist()\n","if 'y_encoded' in numerical_columns:\n","    numerical_columns.remove('y_encoded')\n","\n","# Create a list of columns for the correlation matrix, including numerical features and the target\n","columns_for_correlation = numerical_columns + ['y_encoded']\n","\n","# Calculate the correlation matrix\n","correlation_matrix = df[columns_for_correlation].corr()\n","\n","# Display the correlation matrix using a heatmap\n","plt.figure(figsize=(20, 15))\n","sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Matrix of Numerical Features (Including Engineered Features)')\n","plt.show()\n","\n","# Identify highly correlated pairs (absolute correlation > 0.8)\n","highly_correlated_pairs = []\n","upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n","# Exclude the target variable from the check for highly correlated pairs among features\n","feature_columns = [col for col in upper_triangle.columns if col != 'y_encoded']\n","\n","for i in range(len(feature_columns)):\n","    for j in range(i+1, len(feature_columns)):\n","         if abs(upper_triangle.loc[feature_columns[i], feature_columns[j]]) > 0.8:\n","            highly_correlated_pairs.append((feature_columns[i], feature_columns[j], upper_triangle.loc[feature_columns[i], feature_columns[j]]))\n","\n","\n","print(\"\\nHighly correlated pairs among features (absolute correlation > 0.8):\")\n","for pair in highly_correlated_pairs:\n","    print(f\"{pair[0]} and {pair[1]}: {pair[2]:.4f}\")\n","\n","# Display correlations with the target variable 'y_encoded'\n","\n","if 'y_encoded' in df.columns:\n","    # The correlation of all columns with 'y_encoded' is already in the last column of the correlation_matrix\n","    target_correlation = correlation_matrix['y_encoded'].sort_values(ascending=False)\n","\n","    print(\"\\nCorrelation with target variable 'y_encoded':\")\n","    display(target_correlation)\n","else:\n","    print(\"\\nTarget variable 'y_encoded' not found in DataFrame columns.\")"]},{"cell_type":"markdown","metadata":{"id":"d3b9a8f3"},"source":["### Summary: Feature Review\n","\n","### Data Analysis Key Findings\n","\n","* The correlation matrix of numerical features, including the engineered features, was calculated and visualized.\n","* Several highly correlated pairs of features (absolute correlation > 0.8) were identified, indicating potential multicollinearity:\n","    * `campaign` and `contact_frequency_score` (0.9174)\n","    * `campaign` and `cons_conf_campaign_interaction` (-0.9852)\n","    * `pdays` and `was_previously_contacted` (-0.9895)\n","    * `nr_employed` and `economic_stability_index` (0.9924)\n","    * `nr_employed` and `risk_environment_score` (0.9951)\n","    * `contact_frequency_score` and `cons_conf_campaign_interaction` (-0.9042)\n","    * `economic_stability_index` and `risk_environment_score` (0.9794)\n","* Correlation with the target variable `y_encoded` was calculated and displayed. The features with the highest absolute correlations with `y_encoded` include `cons_conf_idx` (0.0415), `age` (-0.0291), `age_campaign_interaction` (-0.0176), and `previous_success` (-0.0104). Most of the correlations are weak.\n","\n","### Insights or Next Steps\n","\n","* The presence of highly correlated features (multicollinearity) needs to be addressed before training models that are sensitive to it (e.g., Logistic Regression, linear models). This can be done through feature selection (removing one of the highly correlated features) or dimensionality reduction techniques (like PCA).\n","* Although most features show weak linear correlation with the target variable, this does not necessarily mean they are not important for prediction, as non-linear relationships or interactions with other features might exist.\n","* Proceed with separating features and the target variable and splitting the data for model training and evaluation."]},{"cell_type":"markdown","metadata":{"id":"AasJGs_LbLhj"},"source":["## Step 8: Model"]},{"cell_type":"markdown","metadata":{"id":"17b03cd2"},"source":["### Separate Features and Target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"205a5a41"},"outputs":[],"source":["# Separate features (X) and target (y)\n","X = df.drop('y_encoded', axis=1)\n","y = df['y_encoded']\n","\n","# Identify and remove duplicate columns in X\n","duplicate_columns = X.columns[X.columns.duplicated()]\n","if len(duplicate_columns) > 0:\n","    print(f\"Warning: Duplicate columns found in X: {list(duplicate_columns)}\")\n","    X = X.loc[:,~X.columns.duplicated()]\n","    print(\"Duplicate columns removed.\")"]},{"cell_type":"markdown","metadata":{"id":"ad81cf69"},"source":["### Split Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEl9bc3oxRKK"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Separate features and target\n","X = df.drop('y_encoded', axis=1)\n","y = df['y_encoded']\n","\n","# Create train, validation, and test sets\n","X_temp, X_test, y_temp, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",")\n","\n","print(f\"Training set: {X_train.shape}\")\n","print(f\"Validation set: {X_val.shape}\")\n","print(f\"Test set: {X_test.shape}\")\n","print(f\"\\nClass distribution in train: {y_train.value_counts(normalize=True)}\")"]},{"cell_type":"markdown","metadata":{"id":"3cda87d1"},"source":["### Define Preprocessing Steps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DT3JGs2lxRKL"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","\n","# Identify column types from training data\n","numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n","categorical_cols = X_train.select_dtypes(include='object').columns.tolist()\n","\n","# Create preprocessing pipeline\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_cols),\n","        ('cat', Pipeline([\n","            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","        ]), categorical_cols)\n","    ],\n","    remainder='drop'\n",")\n","\n","print(f\"Preprocessing pipeline created\")\n","print(f\"  Numerical features: {len(numerical_cols)}\")\n","print(f\"  Categorical features: {len(categorical_cols)}\")"]},{"cell_type":"markdown","metadata":{"id":"sk2oYyuAZrhk"},"source":["### Evaluate using Multiple Models"]},{"cell_type":"code","metadata":{"id":"32967866"},"source":["%pip install xgboost"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dI4gYzpkaA8t"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","import time\n","\n","print(\"Additional models imported successfully\")"]},{"cell_type":"markdown","metadata":{"id":"wIIcz6lmaDIc"},"source":["### Create Model Comparison Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"463Qn_vhaBzp"},"outputs":[],"source":["def evaluate_model(model, model_name, X_train, X_test, y_train, y_test, preprocessing_pipeline):\n","    \"\"\"\n","    Train and evaluate a model with preprocessing pipeline\n","    \"\"\"\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Training {model_name}...\")\n","    print(f\"{'='*60}\")\n","\n","    # Create pipeline\n","    pipeline = Pipeline(steps=[\n","        ('preprocessor', preprocessing_pipeline),\n","        ('classifier', model)\n","    ])\n","\n","    # Train\n","    start_time = time.time()\n","    pipeline.fit(X_train, y_train)\n","    training_time = time.time() - start_time\n","\n","    # Predict\n","    y_pred = pipeline.predict(X_test)\n","    y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, 'predict_proba') else None\n","\n","    # Calculate metrics\n","    metrics = {\n","        'Model': model_name,\n","        'Accuracy': accuracy_score(y_test, y_pred),\n","        'Precision': precision_score(y_test, y_pred, zero_division=0),\n","        'Recall': recall_score(y_test, y_pred),\n","        'F1-Score': f1_score(y_test, y_pred, zero_division=0),\n","        'ROC-AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None,\n","        'Training Time (s)': round(training_time, 2)\n","    }\n","\n","    # Print results\n","    print(f\"\\n{model_name} Results:\")\n","    print(f\"  Accuracy:  {metrics['Accuracy']:.4f}\")\n","    print(f\"  Precision: {metrics['Precision']:.4f}\")\n","    print(f\"  Recall:    {metrics['Recall']:.4f}\")\n","    print(f\"  F1-Score:  {metrics['F1-Score']:.4f}\")\n","    if metrics['ROC-AUC']:\n","        print(f\"  ROC-AUC:   {metrics['ROC-AUC']:.4f}\")\n","    print(f\"  Training Time: {metrics['Training Time (s)']}s\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    cm = confusion_matrix(y_test, y_pred)\n","    display(pd.DataFrame(cm,\n","                        columns=['Predicted No', 'Predicted Yes'],\n","                        index=['Actual No', 'Actual Yes']))\n","\n","    return metrics, pipeline\n","\n","print(\"Evaluation function created\")"]},{"cell_type":"markdown","metadata":{"id":"AeWhe7lTaTsf"},"source":["### Define Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TI4bDdfxRKN"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(\n","        random_state=42,\n","        class_weight='balanced',\n","        max_iter=1000\n","    ),\n","    'Random Forest': RandomForestClassifier(\n","        n_estimators=100,\n","        max_depth=10,\n","        random_state=42,\n","        class_weight='balanced',\n","        n_jobs=-1\n","    ),\n","    'XGBoost': XGBClassifier(\n","        n_estimators=100,\n","        max_depth=6,\n","        learning_rate=0.1,\n","        random_state=42,\n","        scale_pos_weight=3,\n","        eval_metric='logloss'\n","    ),\n","\n","    'SVM': SVC(\n","        kernel='rbf',\n","        C=1.0,\n","        random_state=42,\n","        class_weight='balanced',\n","        probability=True  # Enable probability predictions for ROC-AUC\n","    )\n","}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nCfhdjI5ahS3"},"source":["### Train All Models"]},{"cell_type":"code","source":["# Train and evaluate on VALIDATION set\n","results = []\n","\n","for name, model in models.items():\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Training {name}...\")\n","\n","    # Create pipeline\n","    pipeline = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('classifier', model)\n","    ])\n","\n","    # Train\n","    pipeline.fit(X_train, y_train)\n","\n","    # Evaluate on VALIDATION set\n","    y_val_pred = pipeline.predict(X_val)\n","    y_val_proba = pipeline.predict_proba(X_val)[:, 1] if hasattr(pipeline, 'predict_proba') else None\n","\n","    # Metrics\n","    f1 = f1_score(y_val, y_val_pred)\n","    auc = roc_auc_score(y_val, y_val_proba) if y_val_proba is not None else None\n","\n","    results.append({\n","        'Model': name,\n","        'F1_Score': f1,\n","        'ROC_AUC': auc\n","    })\n","\n","    print(f\"Validation F1-Score: {f1:.4f}\")\n","    if auc:\n","        print(f\"Validation ROC-AUC: {auc:.4f}\")\n","    print(\"\\nValidation Classification Report:\")\n","    print(classification_report(y_val, y_val_pred))"],"metadata":{"id":"HmA4XtRN2Uwt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btAiWPiEcHE0"},"source":["### Compare Results"]},{"cell_type":"code","source":["# Select best model based on validation F1\n","results_df = pd.DataFrame(results).sort_values('F1_Score', ascending=False)\n","print(\"\\n\" + \"=\"*60)\n","print(\"VALIDATION RESULTS:\")\n","display(results_df)\n","\n","best_model_name = results_df.iloc[0]['Model']\n","print(f\"\\nBest model: {best_model_name}\")"],"metadata":{"id":"51FIw4Um2ab-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URsyHPigcXgU"},"source":["### Select Champion Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2LSlLfocarR"},"outputs":[],"source":["# Select the best model from the dictionary based on the best model name\n","champion_model = models[best_model_name]\n","\n","print(f\"\\nChampion model ({best_model_name}) selected and ready for deployment!\")\n","print(f\"\\nKey Metrics from Validation Set for {best_model_name}:\")\n","\n","champion_metrics = results_df[results_df['Model'] == best_model_name].iloc[0]\n","\n","print(f\"  F1-Score: {champion_metrics['F1_Score']:.4f}\")\n","print(f\"  ROC-AUC: {champion_metrics['ROC_AUC']:.4f}\")\n","# Note: Precision and Recall were not directly stored in results_df in the evaluation loop\n","# If needed, you would re-calculate them for the champion model on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5OQsvaExRKQ"},"outputs":[],"source":["# Train best model on train+validation, evaluate on test\n","print(f\"\\n{'='*60}\")\n","print(f\"Final evaluation of {best_model_name} on test set\")\n","print(\"=\"*60)\n","\n","# Combine train and validation\n","X_train_full = pd.concat([X_train, X_val])\n","y_train_full = pd.concat([y_train, y_val])\n","\n","# Train final model\n","final_pipeline = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('classifier', models[best_model_name])\n","])\n","\n","final_pipeline.fit(X_train_full, y_train_full)\n","\n","# Evaluate on test set (ONLY ONCE)\n","y_test_pred = final_pipeline.predict(X_test)\n","y_test_proba = final_pipeline.predict_proba(X_test)[:, 1]\n","\n","print(\"\\nTest Set Results:\")\n","print(classification_report(y_test, y_test_pred))\n","print(\"\\nConfusion Matrix:\")\n","cm = confusion_matrix(y_test, y_test_pred)\n","display(pd.DataFrame(cm,\n","                     columns=['Predicted No', 'Predicted Yes'],\n","                     index=['Actual No', 'Actual Yes']))\n","\n","print(f\"\\nTest F1-Score: {f1_score(y_test, y_test_pred):.4f}\")\n","print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"ea373cc7"},"source":["### Summary: Evaluating Multiple Models\n","\n","### Key Findings\n","\n","* Multiple classification models were defined and evaluated: Logistic Regression, Random Forest, XGBoost, and Support Vector Machine (SVM). Class weights were adjusted or `scale_pos_weight` was used for these models to address the dataset's class imbalance.\n","* The models were trained on the training data and evaluated on the **validation set**.\n","* Based on the F1-Score on the validation set, the Logistic Regression model achieved the highest F1-Score of 0.3614.\n","* The **champion model (Logistic Regression)** was then trained on the combined training and validation sets and evaluated on the **test set**.\n","* The final evaluation on the test set for the Logistic Regression model shows:\n","    * Accuracy: 0.54\n","    * Precision for 'yes': 0.27\n","    * Recall for 'yes': 0.51\n","    * F1-score for 'yes': 0.35\n","    * ROC-AUC: 0.5453\n","* The confusion matrix on the test set shows:\n","    * True Positives (TP): 633\n","    * False Positives (FP): 1697\n","    * False Negatives (FN): 618\n","    * True Negatives (TN): 2052\n","\n","### Insights and Next Steps\n","\n","* The Logistic Regression model, despite being the champion based on validation F1-score, shows modest performance on the test set, with an F1-score of 0.35. This indicates there is still significant room for improvement in predicting term deposit subscriptions.\n","* The low precision (0.27) suggests that a large proportion of clients predicted to subscribe actually do not, which could lead to wasted marketing efforts. The recall (0.51) indicates that the model is identifying about half of the actual subscribers.\n","* **Hyperparameter Tuning**: While initial tuning was performed, more extensive tuning of the champion model (and potentially other models like XGBoost or Random Forest) could explore a wider range of parameters and potentially improve performance.\n","* **Feature Engineering/Selection**: Revisit the feature engineering and selection process. The weak correlations observed earlier suggest that the current features might not be sufficiently capturing the underlying patterns.\n","* **Explore Advanced Techniques**: Consider more advanced modeling techniques specifically designed for imbalanced datasets or explore ensemble methods.\n","* **Threshold Adjustment**: Investigate adjusting the classification threshold to potentially improve precision or recall depending on the business objective."]},{"cell_type":"markdown","metadata":{"id":"45b5a35d"},"source":["## Step 9: Hyperparameter Tuning and Cross-Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d426d581"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV, StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","import numpy as np\n","\n","# Identify numerical, categorical, and boolean features from X_train\n","# These lists should ideally be defined once earlier in the notebook\n","numerical_cols_in_xtrain = X_train.select_dtypes(include=np.number).columns.tolist()\n","categorical_cols_in_xtrain = X_train.select_dtypes(include='object').columns.tolist()\n","boolean_cols_in_xtrain = X_train.select_dtypes(include='bool').columns.tolist()\n","\n","# Define preprocessing steps\n","# Use the identified column types from X_train\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols_in_xtrain),\n","        ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_cols_in_xtrain),\n","        ('bool', 'passthrough', boolean_cols_in_xtrain)\n","    ],\n","    remainder='passthrough'\n",")\n","\n","\n","# Define the pipeline for tuning (without SMOTE in the main pipeline for now, will address separately if needed)\n","# The Logistic Regression model will use class_weight='balanced' to handle imbalance\n","pipeline_for_tuning = Pipeline(steps=[('preprocessor', preprocessor),\n","                                      ('classifier', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))])\n","\n","\n","# Define the parameter grid for Logistic Regression\n","param_grid = {\n","    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], # Inverse of regularization strength\n","    'classifier__penalty': ['l1', 'l2'] # Regularization type\n","}\n","\n","# Set up Stratified K-Fold Cross-Validation\n","# Use StratifiedKFold to maintain the proportion of the target variable in each fold\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Set up GridSearchCV\n","# Use 'f1' as the scoring metric, as it's suitable for imbalanced datasets\n","grid_search = GridSearchCV(estimator=pipeline_for_tuning,\n","                           param_grid=param_grid,\n","                           scoring='f1', # Optimize for F1-score on the minority class\n","                           cv=cv,\n","                           n_jobs=-1, # Use all available cores\n","                           verbose=2)\n","\n","# Perform GridSearchCV on the training data\n","print(\"Performing GridSearchCV...\")\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and the best score\n","print(\"\\nBest parameters found: \", grid_search.best_params_)\n","print(\"Best cross-validation F1-score: {:.4f}\".format(grid_search.best_score_))\n","\n","# Get the best model from the grid search\n","best_model_tuned = grid_search.best_estimator_\n","\n","print(\"\\nHyperparameter tuning and cross-validation complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"889f5455"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Evaluate the tuned model on the test data\n","y_pred_tuned = best_model_tuned.predict(X_test)\n","\n","# Evaluate the tuned model\n","print(\"\\nModel Evaluation (Tuned Logistic Regression):\")\n","print(f\"Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred_tuned))\n","print(\"\\nConfusion Matrix:\")\n","display(confusion_matrix(y_test, y_pred_tuned))"]},{"cell_type":"markdown","metadata":{"id":"890562a7"},"source":["### Summary: Hyperparameter Tuning and Tuned Model Evaluation\n","\n","#### Key Findings:\n","\n","* Hyperparameter tuning was performed on the Logistic Regression model using GridSearchCV with StratifiedKFold cross-validation.\n","* The optimization metric used was the F1-score, which is appropriate for imbalanced datasets.\n","* The best parameters found for the Logistic Regression model were `{'classifier__C': 0.01, 'classifier__penalty': 'l1'}`.\n","* The best cross-validation F1-score achieved during the tuning process was 0.3540.\n","* Evaluating the tuned model on the test set resulted in an accuracy of 0.5144.\n","* The classification report for the tuned model shows:\n","    * Precision for 'yes': 0.28\n","    * Recall for 'yes': 0.58\n","    * F1-score for 'yes': 0.37\n","* The confusion matrix shows that the tuned model correctly predicted 723 'yes' instances (True Positives) and incorrectly predicted 1900 'yes' instances (False Positives). It also correctly predicted 1849 'no' instances (True Negatives) and incorrectly predicted 528 'no' instances (False Negatives).\n","\n","#### Insights and Next Steps:\n","\n","* Hyperparameter tuning slightly improved the F1-score for the minority class ('yes') on the test set compared to the initial Logistic Regression model (0.37 vs 0.35). The Recall for the 'yes' class also saw an increase (0.58 vs 0.51), while precision decreased (0.28 vs 0.27).\n","* The tuned model still exhibits a trade-off between Precision and Recall, classifying a notable number of False Positives while improving the identification of True Positives.\n","* The overall accuracy decreased compared to the untuned model, but as noted before, accuracy is not the primary metric for imbalanced datasets.\n","* **Further Exploration**: Consider more extensive hyperparameter tuning with a wider range of parameters or different tuning methods (e.g., RandomizedSearchCV) and more folds for cross-validation.\n","* **Alternative Models**: Explore tuning other models (like Random Forest or XGBoost) that showed promising results in the initial comparison, as they might achieve better performance with tuning.\n","* **Feature Importance**: Analyze feature importances from tree-based models (if explored) to understand which features are most influential in predictions.\n","* **Ensemble Methods**: Consider ensemble techniques that combine multiple models to potentially improve overall performance and robustness.\n","* **Threshold Adjustment**: Investigate adjusting the classification threshold of the final model based on business requirements to balance Precision and Recall.\n","* **Final Model Selection**: Based on the performance on the test set and considering the relevant business metrics (Precision, Recall, F1-score, AUC), select the final model for deployment."]},{"cell_type":"markdown","metadata":{"id":"64a83861"},"source":["### Analyze Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b53c9e8b"},"outputs":[],"source":["# Access the trained Logistic Regression model from the pipeline\n","tuned_logistic_regression_model = best_model_tuned.named_steps['classifier']\n","\n","# Get the coefficients from the trained model\n","# The coefficients correspond to the features after preprocessing\n","coefficients = tuned_logistic_regression_model.coef_[0]\n","\n","# Get the feature names after preprocessing\n","# We need to access the feature names from the preprocessor step of the pipeline\n","# The ColumnTransformer's get_feature_names_out() method can provide this\n","feature_names = best_model_tuned.named_steps['preprocessor'].get_feature_names_out()\n","\n","# Create a DataFrame to display feature importances\n","feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n","\n","# Sort features by the absolute value of their coefficients (importance)\n","feature_importance_df['Abs_Coefficient'] = abs(feature_importance_df['Coefficient'])\n","feature_importance_df = feature_importance_df.sort_values('Abs_Coefficient', ascending=False).reset_index(drop=True)\n","\n","print(\"Top 20 most important features based on Logistic Regression coefficients:\")\n","display(feature_importance_df.head(20))\n","\n","# Optional: Visualize the top N feature importances\n","plt.figure(figsize=(10, 8))\n","sns.barplot(x='Abs_Coefficient', y='Feature', hue='Feature', data=feature_importance_df.head(20), palette='viridis', legend=False)\n","plt.title('Top 20 Feature Importances (Absolute Coefficients)')\n","plt.xlabel('Absolute Coefficient Value')\n","plt.ylabel('Feature')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"a00d8a16"},"source":["### Translate Feature Importance to Targeting Recommendations\n","\n","Based on the feature importance analysis (using the absolute coefficients from the Logistic Regression model), we can formulate the following targeting recommendations:\n","\n","*   **Education Level:** The feature `cat__education_professional.course` has the highest absolute coefficient among the top features. This indicates that clients with a professional course education are more likely to subscribe. **Recommendation:** Prioritize targeting individuals with professional course backgrounds. Tailor marketing materials and communication channels to resonate with this educated segment.\n","\n","*   **Consumer Confidence Index:** `num__cons_conf_idx` is another important feature with a positive coefficient. A higher consumer confidence index is associated with a higher likelihood of subscription. **Recommendation:** Time marketing campaigns to coincide with periods of higher consumer confidence. Tailor messaging to reflect positive economic sentiment and how the term deposit aligns with clients' financial optimism.\n","\n","*   **Age:** `num__age` has a negative coefficient among the top features, suggesting that younger individuals (within the scaled range) are slightly more likely to subscribe. **Recommendation:** Consider age as a factor in targeting, potentially focusing on younger adult segments, but balance this with other more influential factors.\n","\n","*   **Economic Indicators:** `num__emp_var_rate` shows some importance. While the direct interpretation might be complex, its positive coefficient suggests a potential link between higher employment variation rates and subscription. Further domain knowledge would be beneficial here. **Recommendation:** Monitor economic indicators, particularly the employment variation rate, and consider their potential influence on campaign timing, although other factors appear more dominant.\n","\n","**Overall Targeting Strategy:** Prioritize clients based on their education level (especially professional courses), consider the prevailing consumer confidence levels when planning campaigns, and use age as a secondary targeting factor. While other economic indicators and features like `pdays`, `previous`, and `campaign` appeared in the top features with very small coefficients (likely due to L1 regularization driving some to zero), their practical importance based on this model's coefficients is minimal compared to education, consumer confidence, and age. Focus initial efforts on segments identified by the most influential features."]},{"cell_type":"markdown","metadata":{"id":"9f8c9006"},"source":["### Analyze Performance Metrics & Estimate ROI Projections\n","\n","To estimate the potential ROI of using the predictive model, we need to consider the model's performance metrics (specifically the confusion matrix) and assume some business parameters:\n","\n","*   **Cost per contact:** The cost associated with contacting a potential client (e.g., agent time, communication costs).\n","*   **Revenue per subscription:** The revenue generated from a successful term deposit subscription.\n","\n","Based on the confusion matrix from the tuned Logistic Regression model evaluated on the test set:\n","\n","*   **True Positives (TP):** Clients who subscribed and were correctly predicted to subscribe. These represent successful targeted contacts.\n","*   **False Positives (FP):** Clients who did not subscribe but were incorrectly predicted to subscribe. These represent wasted targeted contacts.\n","*   **True Negatives (TN):** Clients who did not subscribe and were correctly predicted not to subscribe. These are correctly avoided contacts.\n","*   **False Negatives (FN):** Clients who subscribed but were incorrectly predicted not to subscribe. These represent missed subscription opportunities.\n","\n","We can compare the outcome of a targeted campaign using the model versus a random campaign or a campaign targeting all clients.\n","\n","**Scenario 1: Targeted Campaign using the Model**\n","\n","*   Targeted contacts = TP + FP\n","*   Successful subscriptions = TP\n","*   Total cost = (TP + FP) * Cost per contact\n","*   Total revenue = TP * Revenue per subscription\n","*   Net Profit = Total revenue - Total cost\n","\n","**Scenario 2: Random Campaign (targeting a similar number of clients as the model)**\n","\n","*   Assume we target the same number of clients as the model (TP + FP).\n","*   The probability of subscription in the test set is the baseline conversion rate (True Positives + False Negatives) / Total clients = (723 + 528) / 5000 = 1251 / 5000 = 0.2502.\n","*   Expected successful subscriptions = (TP + FP) * Baseline conversion rate\n","*   Total cost = (TP + FP) * Cost per contact\n","*   Total revenue = Expected successful subscriptions * Revenue per subscription\n","*   Net Profit = Total revenue - Total cost\n","\n","Let's define some example business parameters and calculate the estimated ROI for the targeted campaign."]},{"cell_type":"markdown","source":["## Step 10: Executive Summary"],"metadata":{"id":"jcNemnqNB752"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"83ee6ec0"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","# Extract actual confusion matrix values from test results\n","# This assumes y_test and y_test_pred are available from the final evaluation\n","cm = confusion_matrix(y_test, y_test_pred)\n","tn, fp, fn, tp = cm.ravel()\n","\n","# Business parameters\n","cost_per_contact = 1\n","revenue_per_subscription = 10\n","\n","print(f\"Assumed Cost per Contact: {cost_per_contact}\")\n","print(f\"Assumed Revenue per Subscription: {revenue_per_subscription}\")\n","print(\"\\nConfusion Matrix from Tuned Model (Test Set):\")\n","print(f\"  True Positives (TP): {tp}\")\n","print(f\"  False Positives (FP): {fp}\")\n","print(f\"  False Negatives (FN): {fn}\")\n","print(f\"  True Negatives (TN): {tn}\")\n","\n","# Scenario 1: Targeted Campaign using the Model\n","targeted_contacts_model = tp + fp\n","successful_subscriptions_model = tp\n","total_cost_model = targeted_contacts_model * cost_per_contact\n","total_revenue_model = successful_subscriptions_model * revenue_per_subscription\n","net_profit_model = total_revenue_model - total_cost_model\n","\n","print(\"\\n--- Targeted Campaign using the Model ---\")\n","print(f\"Targeted Contacts: {targeted_contacts_model}\")\n","print(f\"Successful Subscriptions: {successful_subscriptions_model}\")\n","print(f\"Total Cost: {total_cost_model}\")\n","print(f\"Total Revenue: {total_revenue_model}\")\n","print(f\"Net Profit: {net_profit_model}\")\n","print(f\"ROI: {(net_profit_model / total_cost_model * 100):.2f}%\")\n","\n","# Scenario 2: Random Campaign\n","total_clients_test = len(y_test)\n","baseline_conversion_rate = (tp + fn) / total_clients_test\n","targeted_contacts_random = targeted_contacts_model\n","expected_subscriptions_random = targeted_contacts_random * baseline_conversion_rate\n","total_cost_random = targeted_contacts_random * cost_per_contact\n","total_revenue_random = expected_subscriptions_random * revenue_per_subscription\n","net_profit_random = total_revenue_random - total_cost_random\n","\n","print(\"\\n--- Random Campaign (Same Number of Contacts) ---\")\n","print(f\"Expected Subscriptions: {expected_subscriptions_random:.2f}\")\n","print(f\"Total Cost: {total_cost_random}\")\n","print(f\"Total Revenue: {total_revenue_random:.2f}\")\n","print(f\"Net Profit: {net_profit_random:.2f}\")\n","print(f\"ROI: {(net_profit_random / total_cost_random * 100):.2f}%\")\n","\n","print(\"\\n--- Model Advantage ---\")\n","print(f\"Additional Profit from Model: {net_profit_model - net_profit_random:.2f}\")\n","print(f\"Improvement: {((net_profit_model - net_profit_random) / net_profit_random * 100):.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"a426ba7a"},"source":["\n","\n","**Project Title:** Bank Marketing Campaign Optimization through Predictive Analytics\n","\n","**Executive Summary:**\n","\n","This project aimed to develop a predictive model to identify individuals most likely to subscribe to a term deposit, thereby optimizing direct marketing campaign effectiveness for a financial institution.\n","\n","The analysis was conducted on the provided bank marketing dataset. Initial data understanding revealed a mix of numerical and categorical features with no missing values but a notable class imbalance in the target variable ('y'), with only about 25% of clients subscribing to a term deposit. Exploratory Data Analysis (EDA) highlighted key relationships, including the influence of economic indicators like consumer confidence index, and previous campaign outcomes on subscription likelihood.\n","\n","Data preprocessing involved handling categorical features (including 'unknown' values), managing outliers, and engineering new features such as customer behavior metrics, economic context indicators, and demographic enhancements. Categorical features were one-hot encoded, and numerical features were scaled.\n","\n","Multiple classification models were evaluated, including Logistic Regression, Random Forest, XGBoost, and SVM, with strategies implemented to address class imbalance (e.g., `class_weight='balanced'`, `scale_pos_weight`). The Logistic Regression model with `class_weight='balanced'` demonstrated the best balance between Precision and Recall for the minority class ('yes'), achieving an F1-score of 0.37 after tuning.\n","\n","Feature importance analysis from the tuned Logistic Regression model indicated that specific education backgrounds (professional courses), consumer confidence levels, and age are among the most influential factors in predicting subscription.\n","\n","Translating these findings into business recommendations suggests prioritizing clients based on their education level (especially professional courses), timing campaigns with favorable economic conditions, and considering age in targeting strategies.\n","\n","An estimated ROI projection based on the tuned model's actual test\n","performance (TP=633, FP=1697, FN=618, TN=2052) indicates a 14.3%\n","improvement in net profit compared to random targeting. With example\n","business parameters (cost=$1/contact, revenue=$10/subscription), the\n","model generates $4,000 profit versus $3,500 for random selection.\n","\n","While the model shows modest predictive power (F1=0.37, Recall=0.51),\n","it demonstrates practical value by identifying approximately half of\n","likely subscribers while reducing wasted contacts by focusing on\n","higher-probability prospects. The business value depends heavily on\n","actual cost/revenue parameters and whether the 14% improvement\n","justifies deployment and maintenance costs.\n","\n","In conclusion, the developed predictive model, particularly the tuned Logistic Regression model, provides a valuable tool for enhancing bank marketing campaigns by enabling more precise customer targeting, leading to improved subscription rates and a positive return on investment. Further refinement through exploring alternative models, advanced techniques, or threshold adjustments could potentially yield even greater improvements."]},{"cell_type":"markdown","metadata":{"id":"95e93ec1"},"source":["## Step 11: Deployment\n","\n","Save the Final Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"da911a20"},"outputs":[],"source":["import joblib\n","import os\n","\n","# Define the filename for the saved model\n","model_filename = 'tuned_logistic_regression_model.joblib'\n","\n","# Save the trained model to a file\n","joblib.dump(best_model_tuned, model_filename)\n","\n","print(f\"Final model saved successfully to {model_filename}\")\n","\n","# Optional: Verify the file exists\n","if os.path.exists(model_filename):\n","    print(f\"File '{model_filename}' found.\")\n","else:\n","    print(f\"File '{model_filename}' not found.\")"]},{"cell_type":"markdown","metadata":{"id":"ac48a226"},"source":["### Create a Prediction Script\n","\n","This script demonstrates how to load the trained model and make predictions on new data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7b074782"},"outputs":[],"source":["import joblib\n","import pandas as pd\n","import numpy as np\n","\n","# Define the filename of the saved model\n","model_filename = 'tuned_logistic_regression_model.joblib'\n","\n","# Define the filename of the raw new data (for context, but we will use processed data for demo)\n","raw_new_data_filename = 'bank_marketing_2024.csv' # Replace with path to new raw data\n","\n","# --- For Demonstration Purposes in Notebook ---\n","# Use a sample of the *processed* DataFrame (df) as the \"new\" data\n","# In a real-world scenario, you would load new raw data and apply preprocessing/feature engineering\n","new_data_sample_size = 10 # Number of rows to use from the processed data for demonstration\n","\n","# In a real prediction script, you would load your new raw data here\n","# For this demonstration, we'll simulate new data by taking a sample from the processed df\n","if new_data_sample_size < len(df):\n","    # Sample from the processed DataFrame and drop the target column (y_encoded)\n","    X_new_raw_demo = df.sample(n=new_data_sample_size, random_state=42).drop('y_encoded', axis=1).copy()\n","else:\n","    X_new_raw_demo = df.drop('y_encoded', axis=1).copy()\n","\n","# In a real scenario, you would apply all preprocessing and feature engineering steps\n","# to X_new_raw_demo to get X_new in the correct format expected by the pipeline.\n","# For this demonstration, X_new_raw_demo already has the correct structure.\n","X_new = X_new_raw_demo.copy()\n","# ---------------------------------------------\n","\n","\n","try:\n","    # Load the saved model\n","    loaded_model = joblib.load(model_filename)\n","    print(f\"Model loaded successfully from {model_filename}\")\n","\n","    # --- Input Validation ---\n","    # Get the column names the model was trained on from the X_train DataFrame\n","    # Assuming X_train is available in the environment from the training step\n","    expected_columns = X_train.columns.tolist()\n","    new_data_columns = X_new.columns.tolist()\n","\n","    # Check for missing columns in the new data\n","    missing_columns = [col for col in expected_columns if col not in new_data_columns]\n","    if missing_columns:\n","        print(f\"\\nWarning: Missing columns in new data: {missing_columns}\")\n","        # In a real scenario, you might want to handle these missing columns\n","        # (e.g., add them and impute) or raise an error. For this demo, we'll just warn.\n","        # Example handling:\n","        # for col in missing_columns:\n","        #     X_new[col] = 0 # Or use a suitable default/imputed value\n","\n","    # Check for extra columns in the new data\n","    extra_columns = [col for col in new_data_columns if col not in expected_columns]\n","    if extra_columns:\n","        print(f\"\\nWarning: Extra columns in new data: {extra_columns}\")\n","        # In a real scenario, you would typically drop these extra columns\n","        X_new = X_new.drop(columns=extra_columns)\n","        print(\"Extra columns dropped.\")\n","\n","    # Reindex the new data to match the order of the training data columns\n","    # This is important for consistent input to the pipeline\n","    X_new = X_new.reindex(columns=expected_columns, fill_value=0) # Fill missing with 0 for demo, adjust as needed\n","\n","    print(\"\\nInput validation complete.\")\n","    # ------------------------\n","\n","\n","    # Make predictions on the new data\n","    # The loaded pipeline will automatically apply the necessary preprocessing steps\n","    predictions = loaded_model.predict(X_new)\n","\n","    # Add predictions to the new data DataFrame (or the original new_data_for_prediction)\n","    # Since X_new was modified, let's add predictions back to the original sample for clarity\n","    X_new_raw_demo['predicted_subscription'] = predictions\n","\n","\n","    print(\"\\nPredictions on demonstration data:\")\n","    # Display some original-like columns (need to map back from processed if necessary)\n","    # For simplicity, let's display some columns from the demonstration data along with the prediction\n","    # Note: 'duration' column was removed, so we display 'age' and 'campaign' instead.\n","    display(X_new_raw_demo[['age', 'campaign', 'predicted_subscription']].head())\n","\n","    # In a real scenario, you would load your raw new data, apply preprocessing and feature engineering,\n","    # and then use the loaded_model.predict() on that fully prepared new data.\n","\n","except FileNotFoundError:\n","    print(f\"Error: Model file '{model_filename}' not found.\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","    import traceback\n","    traceback.print_exc() # Print full traceback for debugging"]},{"cell_type":"markdown","metadata":{"id":"727440cb"},"source":["### Summary: Prediction Script Results\n","\n","The prediction script demonstrates how to load the trained Logistic Regression model and use it to make predictions on new data.\n","\n","For the small sample of processed data used in the demonstration (10 rows):\n","\n","*   The script successfully loaded the saved model (`tuned_logistic_regression_model.joblib`).\n","*   It applied the necessary preprocessing steps implicitly through the loaded pipeline.\n","*   It generated predictions (`predicted_subscription`) for each row in the sample. The output table shows the 'age', 'campaign', and the predicted subscription status (0 for no, 1 for yes) for each of the sampled clients.\n","\n","This script serves as a basic example of how the deployed model can be used in a real-world scenario to predict term deposit subscriptions for new clients."]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V5E1","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}